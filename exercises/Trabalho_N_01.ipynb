{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lima-breno/deep_learning_and_neural_networks/blob/main/Trabalho_em_Grupo_N_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trabalho N.01**\n"
      ],
      "metadata": {
        "id": "DbMUcdkzVFIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Conceitue e relacione: aprendizado de máquina, aprendizado de representação e deep learning**"
      ],
      "metadata": {
        "id": "KSyDF7K6Vrls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conceito:**\n",
        "\n",
        "**Aprendizado de Máquina (Machine Learning)**: É uma sub-área da inteligência artificial que foca em desenvolver algoritmos que permitem que computadores aprendam a partir de dados. Esses algoritmos constroem modelos matemáticos baseados em dados de treinamento para fazer previsões ou tomar decisões sem serem explicitamente programados para executar uma tarefa específica.\n",
        "\n",
        "**Aprendizado de Representação**: Dentro do aprendizado de máquina, o aprendizado de representação foca em encontrar maneiras de representar dados que tornem mais fácil a extração de informações úteis. Em vez de usar características predefinidas, o aprendizado de representação busca aprender características diretamente dos dados, o que é especialmente útil em tarefas complexas como reconhecimento de imagens e processamento de linguagem natural.\n",
        "\n",
        "**Deep Learning**: É uma sub-área do aprendizado de máquina que utiliza redes neurais profundas para aprendizado de representação. Redes neurais com múltiplas camadas permitem a modelagem de dados com complexidade elevada, extraindo automaticamente características a diferentes níveis de abstração. Deep Learning é particularmente poderoso em tarefas como reconhecimento de fala, visão computacional e tradução automática.\n",
        "\n",
        "**Relacionamento:**\n",
        "\n",
        "**Aprendizado de Máquina** é o campo mais amplo que inclui o desenvolvimento de algoritmos para aprendizado a partir de dados.\n",
        "\n",
        "**Aprendizado de Representação** é uma parte crítica do aprendizado de máquina, focando em como os dados devem ser representados para facilitar o aprendizado.\n",
        "\n",
        "**Deep Learning** é uma abordagem dentro do aprendizado de máquina que usa redes neurais profundas para realizar aprendizado de representação, permitindo que modelos aprendam diretamente dos dados brutos sem a necessidade de engenharia de características manual."
      ],
      "metadata": {
        "id": "YrX56qJiXDy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Conceitue função de perda e descreva seu uso na obtenção de um bom modelo.**"
      ],
      "metadata": {
        "id": "CKLml5KCVyRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Função de Perda (Loss Function)**: É uma medida que quantifica o quanto as previsões de um modelo estão distantes dos valores reais esperados. A função de perda calcula o erro do modelo em relação aos dados de treinamento, e seu objetivo é ser minimizada durante o treinamento do modelo. Por exemplo, em problemas de regressão, uma função de perda comum é o erro quadrático médio (MSE), enquanto em problemas de classificação é comum usar a entropia cruzada.\n",
        "\n",
        "Durante o treinamento, a função de perda é utilizada para ajustar os pesos do modelo. Otimizadores, como o gradiente descendente, usam o valor da função de perda para atualizar os pesos na direção que minimiza o erro. Um bom modelo é aquele que tem uma função de perda baixa em dados de teste, indicando que ele generaliza bem e faz previsões precisas em dados não vistos."
      ],
      "metadata": {
        "id": "wSWIlyfaZ6hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Forneça o pseudocódigo (algo em torno de 4~5 linhas) para o algoritmo “SGD com minibatch” e\n",
        "descreva sucintamente cada um de seus passos.**"
      ],
      "metadata": {
        "id": "wYCyNoSPV1af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    data_batch = sample_training_data(data, 256)  # (Passo 1) Amostra minibatch\n",
        "    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)  # (Passo 2) Calcula gradiente da perda\n",
        "    weights += - step_size * weights_grad  # (Passo 3) Atualização dos pesos\n",
        "\n",
        "'''\n",
        "Passo 01: Amostra minibatch:\n",
        "este passo faz a seleção aleatória de um subconjunto de dados de treinamento. Neste caso foram 256 exemplos\n",
        "\n",
        "Passo 02: Calcula o gradiente de perda\n",
        "este passo faz a avaliação do gradiente da função de perda em relação aos parâmetros (weights), usando apenas o minibatch.\n",
        "A ideia é que o gradiente aponte a direção de maior aumento da perda.\n",
        "\n",
        "Passo 03: Atualização dos pesos\n",
        "este passo faz os ajustes dos pesos  na direção oposta ao gradiente (descida), escalados por um fator chamado step_size (ou taxa de aprendizado).\n",
        "'''"
      ],
      "metadata": {
        "id": "dkYiFtmx2hPv",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b267a0c8-5323-4276-a5b4-cc9271f50ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sample_training_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fb23473f435d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (Passo 1) Amostra minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mweights_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (Passo 2) Calcula gradiente da perda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights_grad\u001b[0m  \u001b[0;31m# (Passo 3) Atualização dos pesos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sample_training_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descrição dos passos:\n",
        "### Inicialize os pesos W\n",
        "Os pesos da rede (parâmetros) são iniciados com valores aleatórios ou com algum método como He/Xavier.\n",
        "\n",
        "### Para cada época:\n",
        "Percorremos todo o conjunto de dados uma vez (isso é uma época de treinamento).\n",
        "\n",
        "### Divida os dados em mini-batches:\n",
        "Em vez de usar um exemplo (SGD puro) ou todo o conjunto (batch gradient descent), dividimos em pequenos grupos (mini-batches), o que equilibra estabilidade e velocidade.\n",
        "\n",
        "### Para cada mini-batch:\n",
        "Iteramos sobre os mini-batches da época atual.\n",
        "\n",
        "### Calcule o gradiente da função de perda no mini-batch:\n",
        "Usamos apenas os dados do mini-batch para estimar como os pesos devem ser ajustados.\n",
        "\n",
        "### Atualize os pesos:\n",
        "Aplicamos a fórmula do gradiente descendente: W = W - η * gradiente, onde η (eta) é a taxa de aprendizado.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bvl79xFS2jEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Construa o grafo de computação para a função f(x, y, z, w) = 2 × [(x × y) + max(z, w)]. Em seguida,\n",
        "realize o forward pass considerando a entrada (3, -4, 2, -1) e calcule as derivadas em relação a cada entrada\n",
        "usando backpropagation**"
      ],
      "metadata": {
        "id": "U7gWzQQoV4NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz graphviz > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "-qJ8kEJT1LNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando bibliotecas\n",
        "import torch\n",
        "from torchviz import make_dot\n",
        "from graphviz import Source\n",
        "from IPython.display import display\n",
        "\n",
        "# Definindo os tensores com os valores especificados no enunciado\n",
        "## O parâmetro requires_grade = True foi utilizado paara calcular gradientes\n",
        "## automaticamente durante o \"backward pass\"\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = torch.tensor(-4.0, requires_grad=True)\n",
        "z = torch.tensor(2.0, requires_grad=True)\n",
        "w = torch.tensor(-1.0, requires_grad=True)\n",
        "\n",
        "# Forward pass: processo de calcular o valor da função f passo a passo,\n",
        "## conforme definido pela seguinte expressão matemática:\n",
        "## f(x, y, z, w) = 2 × [(x × y) + max(z, w)]\n",
        "\n",
        "v1 = x * y\n",
        "v2 = torch.max(z, w)\n",
        "v3 = v1 + v2\n",
        "f = 2 * v3\n",
        "\n",
        "'''\n",
        "Para a realização do forward pass e definir o valor da função dada a entrada (3,-4,2,-1)\n",
        "é necessário realizar os seguintes passos:\n",
        "v1 = (3 * -4)  = -12\n",
        "v2 = max(2,-1) = 2\n",
        "v3 = -12 + 2 = -10\n",
        "f = 2 * -10 = -20\n",
        "'''\n",
        "# Backward pass: processo de calcular as derivadas parciais da saída (f)\n",
        "## em relação às entradas x,y,z,w. O comando f.backward executa a retropropagação\n",
        "## no grafo computacional, calculando automaticamente os gradientes de f em cada\n",
        "## tensor estabelecido anteriormente. Os tensores são armazenados nos atributos .grad\n",
        "\n",
        "f.backward()\n",
        "\n",
        "\n",
        "'''\n",
        "Para a realização do backward pass temos que primeiro fazer uma série de derivações\n",
        "\n",
        "\n",
        "- Em relação à função f = 2 * v3 -> df/dv3 = 2\n",
        "\n",
        "- Em relação à função v3 = v1 + v2 -> dv3/dv1 = 1 e dv3/dv2 = 1\n",
        " Aplicando a regra da cadeia (considerando as derivadas anteriores)\n",
        "  df/dv1 = df/dv3 * dv3/dv1 = 2 * 1 = 2\n",
        "  df/dv2 = 2 * 1 = 2\n",
        "\n",
        "- Em relação à função v1 = x * y\n",
        "  dv1/dx = y = -4\n",
        "  dv1/dy = x = 3\n",
        "  Fazendo a propagação\n",
        "  df/dx = df/dv1 * dv1/dx = 2 * (-4) = -8\n",
        "\n",
        "\n",
        "- Em relação à função v2 = max(z,w), como z(2) é maior que w(-1):\n",
        "  dv2/dq = 1 e dv2/dw = 0\n",
        "  Aplicando a regra da cadeia\n",
        "  df/dz = df/dv2*dv2/dz = 2 * 1 = 2\n",
        "  df/dw = 2 * 0 = 0\n",
        "\n",
        "- Assim, os gradiente finais serão os seguintes:\n",
        "  df/dx = -8\n",
        "  df/dy = 6\n",
        "  df/dz = 2\n",
        "  df/dw = 0\n",
        "\n",
        "'''\n",
        "# Gerar o grafo computacional:\n",
        "dot = make_dot(f, params={\"x\": x, \"y\": y, \"z\": z, \"w\": w, \"v1\": v1, \"v2\": v2, \"v3\": v3})\n",
        "\n",
        "\n",
        "# Imprimindo o grafo\n",
        "dot.format = \"png\"\n",
        "dot.attr(rankdir = 'LR')\n",
        "dot.attr(ranksep='1.0')\n",
        "dot.attr(nodep = '0.5')\n",
        "\n",
        "graph = Source(dot.source)\n",
        "display(graph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "KgsxDepR1OSL",
        "outputId": "38324808-d5f2-4629-f526-e2ca9410fac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"864pt\" height=\"181pt\"\n viewBox=\"0.00 0.00 864.00 181.16\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 178)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-178 864,-178 864,4 -4,4\"/>\n<!-- 133172488161424 -->\n<g id=\"node1\" class=\"node\">\n<title>133172488161424</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"860,-102.5 806,-102.5 806,-71.5 860,-71.5 860,-102.5\"/>\n<text text-anchor=\"middle\" x=\"833\" y=\"-78.5\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 133172487883600 -->\n<g id=\"node2\" class=\"node\">\n<title>133172487883600</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"734,-96.5 645,-96.5 645,-77.5 734,-77.5 734,-96.5\"/>\n<text text-anchor=\"middle\" x=\"689.5\" y=\"-84.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 133172487883600&#45;&gt;133172488161424 -->\n<g id=\"edge12\" class=\"edge\">\n<title>133172487883600&#45;&gt;133172488161424</title>\n<path fill=\"none\" stroke=\"black\" d=\"M734.11,-87C753.75,-87 776.68,-87 795.41,-87\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"795.61,-90.5 805.61,-87 795.61,-83.5 795.61,-90.5\"/>\n</g>\n<!-- 133172487888016 -->\n<g id=\"node3\" class=\"node\">\n<title>133172487888016</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"573,-96.5 484,-96.5 484,-77.5 573,-77.5 573,-96.5\"/>\n<text text-anchor=\"middle\" x=\"528.5\" y=\"-84.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 133172487888016&#45;&gt;133172487883600 -->\n<g id=\"edge1\" class=\"edge\">\n<title>133172487888016&#45;&gt;133172487883600</title>\n<path fill=\"none\" stroke=\"black\" d=\"M573.37,-87C592.31,-87 614.66,-87 634.6,-87\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"634.77,-90.5 644.77,-87 634.77,-83.5 634.77,-90.5\"/>\n</g>\n<!-- 133172487891184 -->\n<g id=\"node4\" class=\"node\">\n<title>133172487891184</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"400,-120.5 311,-120.5 311,-101.5 400,-101.5 400,-120.5\"/>\n<text text-anchor=\"middle\" x=\"355.5\" y=\"-108.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 133172487891184&#45;&gt;133172487888016 -->\n<g id=\"edge2\" class=\"edge\">\n<title>133172487891184&#45;&gt;133172487888016</title>\n<path fill=\"none\" stroke=\"black\" d=\"M400.09,-104.88C422.6,-101.72 450.27,-97.84 474.05,-94.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"474.57,-97.96 483.99,-93.11 473.6,-91.03 474.57,-97.96\"/>\n</g>\n<!-- 133172487891088 -->\n<g id=\"node5\" class=\"node\">\n<title>133172487891088</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"227,-162.5 126,-162.5 126,-143.5 227,-143.5 227,-162.5\"/>\n<text text-anchor=\"middle\" x=\"176.5\" y=\"-150.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 133172487891088&#45;&gt;133172487891184 -->\n<g id=\"edge3\" class=\"edge\">\n<title>133172487891088&#45;&gt;133172487891184</title>\n<path fill=\"none\" stroke=\"black\" d=\"M217.66,-143.47C243.37,-137.37 276.93,-129.41 304.38,-122.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"305.27,-126.28 314.19,-120.57 303.65,-119.47 305.27,-126.28\"/>\n</g>\n<!-- 133172488160752 -->\n<g id=\"node6\" class=\"node\">\n<title>133172488160752</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"54,-174 0,-174 0,-144 54,-144 54,-174\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-162\" font-family=\"monospace\" font-size=\"10.00\">x</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-151\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 133172488160752&#45;&gt;133172487891088 -->\n<g id=\"edge4\" class=\"edge\">\n<title>133172488160752&#45;&gt;133172487891088</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.03,-157.94C71.17,-157.24 94.32,-156.3 115.82,-155.43\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"116.03,-158.92 125.88,-155.02 115.75,-151.93 116.03,-158.92\"/>\n</g>\n<!-- 133172487891136 -->\n<g id=\"node7\" class=\"node\">\n<title>133172487891136</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"227,-120.5 126,-120.5 126,-101.5 227,-101.5 227,-120.5\"/>\n<text text-anchor=\"middle\" x=\"176.5\" y=\"-108.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 133172487891136&#45;&gt;133172487891184 -->\n<g id=\"edge5\" class=\"edge\">\n<title>133172487891136&#45;&gt;133172487891184</title>\n<path fill=\"none\" stroke=\"black\" d=\"M227.28,-111C250.17,-111 277.36,-111 300.72,-111\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"300.82,-114.5 310.82,-111 300.82,-107.5 300.82,-114.5\"/>\n</g>\n<!-- 133172488160848 -->\n<g id=\"node8\" class=\"node\">\n<title>133172488160848</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"54,-126 0,-126 0,-96 54,-96 54,-126\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-114\" font-family=\"monospace\" font-size=\"10.00\">y</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-103\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 133172488160848&#45;&gt;133172487891136 -->\n<g id=\"edge6\" class=\"edge\">\n<title>133172488160848&#45;&gt;133172487891136</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.03,-111C71.17,-111 94.32,-111 115.82,-111\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"115.88,-114.5 125.88,-111 115.88,-107.5 115.88,-114.5\"/>\n</g>\n<!-- 133172487887920 -->\n<g id=\"node9\" class=\"node\">\n<title>133172487887920</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"412,-72.5 299,-72.5 299,-53.5 412,-53.5 412,-72.5\"/>\n<text text-anchor=\"middle\" x=\"355.5\" y=\"-60.5\" font-family=\"monospace\" font-size=\"10.00\">MaximumBackward0</text>\n</g>\n<!-- 133172487887920&#45;&gt;133172487888016 -->\n<g id=\"edge7\" class=\"edge\">\n<title>133172487887920&#45;&gt;133172487888016</title>\n<path fill=\"none\" stroke=\"black\" d=\"M412.02,-70.79C431.87,-73.58 454.18,-76.71 473.87,-79.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"473.51,-82.96 483.9,-80.88 474.48,-76.02 473.51,-82.96\"/>\n</g>\n<!-- 133172487881824 -->\n<g id=\"node10\" class=\"node\">\n<title>133172487881824</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"227,-72.5 126,-72.5 126,-53.5 227,-53.5 227,-72.5\"/>\n<text text-anchor=\"middle\" x=\"176.5\" y=\"-60.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 133172487881824&#45;&gt;133172487887920 -->\n<g id=\"edge8\" class=\"edge\">\n<title>133172487881824&#45;&gt;133172487887920</title>\n<path fill=\"none\" stroke=\"black\" d=\"M227.28,-63C246.34,-63 268.4,-63 288.74,-63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"288.88,-66.5 298.88,-63 288.88,-59.5 288.88,-66.5\"/>\n</g>\n<!-- 133172488160944 -->\n<g id=\"node11\" class=\"node\">\n<title>133172488160944</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"54,-78 0,-78 0,-48 54,-48 54,-78\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-66\" font-family=\"monospace\" font-size=\"10.00\">z</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-55\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 133172488160944&#45;&gt;133172487881824 -->\n<g id=\"edge9\" class=\"edge\">\n<title>133172488160944&#45;&gt;133172487881824</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.03,-63C71.17,-63 94.32,-63 115.82,-63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"115.88,-66.5 125.88,-63 115.88,-59.5 115.88,-66.5\"/>\n</g>\n<!-- 133172487890176 -->\n<g id=\"node12\" class=\"node\">\n<title>133172487890176</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"227,-30.5 126,-30.5 126,-11.5 227,-11.5 227,-30.5\"/>\n<text text-anchor=\"middle\" x=\"176.5\" y=\"-18.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 133172487890176&#45;&gt;133172487887920 -->\n<g id=\"edge10\" class=\"edge\">\n<title>133172487890176&#45;&gt;133172487887920</title>\n<path fill=\"none\" stroke=\"black\" d=\"M217.66,-30.53C243.37,-36.63 276.93,-44.59 304.38,-51.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"303.65,-54.53 314.19,-53.43 305.27,-47.72 303.65,-54.53\"/>\n</g>\n<!-- 133172488161040 -->\n<g id=\"node13\" class=\"node\">\n<title>133172488161040</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"54,-30 0,-30 0,0 54,0 54,-30\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-18\" font-family=\"monospace\" font-size=\"10.00\">w</text>\n<text text-anchor=\"middle\" x=\"27\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 133172488161040&#45;&gt;133172487890176 -->\n<g id=\"edge11\" class=\"edge\">\n<title>133172488161040&#45;&gt;133172487890176</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.03,-16.06C71.17,-16.76 94.32,-17.7 115.82,-18.57\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"115.75,-22.07 125.88,-18.98 116.03,-15.08 115.75,-22.07\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x791ea3673910>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) Descreva como deve se comportar um “bom” método de inicialização de pesos, abordando os problemas\n",
        "de quebra de simetria e saturação. Fale sobre alguma abordagem adequada para isso**"
      ],
      "metadata": {
        "id": "nbGYtxZdV6xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A inicialização dos pesos é fundamental para garantir o bom desempenho e a estabilidade do treinamento de redes neurais profundas. Um método de inicialização eficaz deve cumprir dois objetivos essenciais: quebrar a simetria e evitar a saturação das ativações e dos gradientes.\n",
        "\n",
        "A quebra de simetria é necessária porque, se todos os neurônios de uma camada forem inicializados com os mesmos valores de peso, eles produzirão as mesmas saídas e receberão os mesmos gradientes durante a retropropagação. Isso impediria que cada neurônio aprendesse representações distintas dos dados, tornando a rede ineficaz. Para evitar esse problema, é essencial inicializar os pesos com valores aleatórios, garantindo que cada neurônio comece com comportamentos ligeiramente diferentes e, assim, aprenda diferentes aspectos dos dados.\n",
        "\n",
        "Além disso, é necessário evitar a saturação das funções de ativação. Funções como o sigmoid e o tanh tendem a saturar para entradas muito grandes ou muito pequenas, produzindo gradientes próximos de zero. Quando isso acontece, o treinamento se torna extremamente lento ou até mesmo inviável, pois os pesos deixam de ser atualizados de maneira eficaz (fenômeno conhecido como desvanecimento do gradiente). Em contrapartida, se os pesos forem muito grandes, pode ocorrer o problema da explosão do gradiente, tornando o treinamento instável. Assim, os pesos devem ser inicializados de forma que a variância das ativações e dos gradientes permaneça controlada à medida que os sinais atravessam as camadas da rede.\n",
        "\n",
        "Um dos métodos mais conhecidos e eficazes para atender a esses requisitos é a inicialização de Xavier. Essa técnica visa manter a variância constante das ativações e dos gradientes durante o fluxo de dados para frente (forward pass) e para trás (backpropagation) na rede. Os pesos devem ser sorteados de maneira aleatória com uma variância que depende do número de entradas e o número de saídas conforme imagem abaixo:\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAqsAAAC9CAYAAACUCzReAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAC92SURBVHhe7d19cBvnfSfwr6+eM3IqAOfciG5cGz11ZFzpkKIvoqSLZekayVCYmnQVA6RsyVQimEwmFq1cLaVOYUqxaFwdy8pYgpRrKfMiMbpYFNhTTbZmCSd2weRsk0xjEqx4XvnMCsqpETSnq8CJztCEN3t/LF52n10AS5AUl9T3M8PRaF+effbZZx/88DzPLm6RZVkGEREREZEF/QtxARERERGRVTBYJSIiIiLLYrBKRERERJbFYJWIiIiILIvBKhERERFZFoNVIiIiIrIsBqtEREREZFkMVomIiIjIshisEhEREZFlMVglIiIiIstisEpERERElsVglYiIiIgsi8EqEREREVkWg1UiIiIisiwGq0RERERkWQxWiYiIiMiyGKwSERERkWUxWCUiIiIiy2KwSkRERESWxWCViIiIiCyLwSoRERERWRaDVSIiIiKyrFtkWZbFhURERItVaiKCY8d6IF0DnL/nQf22Oqy4Q9yKiKyCPatERHTTSPb58eB/HIT7iVbs+coqJL+/DQ/+m+XY2ZcUNyUii2DPKhER3Rymotj96QCc/X+D1vudyrJkGN67/YjYfOj+xw54log7EdF8Y88qERHdHC7EEE3FcGB9AJHMMqcPjV4AqTB63tFuTkTWwGCViIhuDvd40PgHZXDXroJbtdh2q/Lvx1OqhURkGZwGQEREN7FRtC1/EAcS63Hkw140lonriWi+sWeViIhuWsnewwglgMq2lxmoElkUe1aJiOjmlIzA/xkvpCd/hL/ZtwrpR66IyGIYrBIR0c1nSkJow8OIPP7XCH/VDZu4nogsg9MAiIjo5pIOVEd2/wy96UA19W4InTFxQyKyAgarRER0E0ki8rQf0rM/Q0dtbuA/9t9eR4Ldq0SWxGkARESWkELyCuC8Y4YR07Ukkr/hhHOGySxOSQw9/4fYeCiOsjJtASUvrsDRX3bDxx8FILIc9qwSEc27JKJ7HkZg6Lq4YvqmBhHYuBMR/nqo3kQnnjoYA6aSSFxMaP5SZStwLwNVIktizyoR3bwudsL7H3YjkkjlltnW4+Wf9aL5nsyCUbT9/oM4cDG7ARq7LuBIzex1XcaPrsXG89/FhwdWiatK834bqr9xO07+uAXu9AvviYgWKgarRHTTix+qRkWrBHfbGIZ3ucTVACQc+MxGfND2MxzdXDa7T45f7ERtRQSNvzg5q0PQ0T9eit2u4TznQ0S0cHAaABHd9Fz3VgIArl69Kq4CACR7X0JnzV+iY7YDVQDRg7sx+OWmWQ1UAWD9ky1ItQYQviauISJaWBisEhHd+bsoA5C4mBDXAFNDaHv+Ezj47VkaolebiqDzVaDRt15cM3Pldagv60HXG6opDkRECxCDVSKiO25HGQDEJMSFVaPBp3D+T4LwaHo+U0iOR9Hz5ijU011zUogP9CDyfgIpAEglMPpmDyIx4amn4Qh6UYt11drFihQS70fQMyAhNSWsSeYOmpqI6tMFAKzA6oeASF9EXEFEtKAwWCUiuscNNwBMxKHpWz13AE8Nt+CoT/VDnBd7sLOqAtvOXAZ+NYBnNq7F2uUOrD2aC3PjR7ch8P4ldNUvx8Y9bdj2SBsGr15CV+Pd2BbOBZrxoShS5VVYoXsIKonI1zfimYEUBoPVWN46lFvV58fSuzcidAEAhvDcmlp41/oNh/vdbjcQkyCJK4iIFhAGq0REKMPvlgFI/TMuZXsxE+jc04+Gw41KryvSvyW/ZhsGn/hr9AZ8qNvcgld33YlYAnAvSz/INBVF6C0PDu7ahKpPA7ET51F/+gg2Xe5EeAJIXsv1gkpnJcB1Zy79jOGX8FLZUZzc5cbtV4Bk4lJ21dBbvYBtDVbfAwCr0BRQ5tsacd1bCZz7QNdbLBr9zlos//3l5v/WHsComAgR0Rzh2wCIiBBHaH0FAu+7ERwZRssyIBn24g8vBPHTZ9zpbVKINN8D7ykPOn6Ze3I/fnQtKr71SRz5sBeNZQDGQ9j5kzoc8Y/C/6+3YeCZn+DDfSuAiz0IdTtR/9R6lKV7UiPNDnjRjcl2TzYnACAd2o1B78toTLahYs0BrOi4jJM+m/IareUP4sCDJzH5X+qUjad64P/0AL56+WXoZtX2+eFoSKJjnl5273A4xEU0TZOTk+IiopuPTEREcn+TXbbb7fJTEVmWr74u71i3Xx75tWqDSyfkh+122V5/Wv44u/Bj+XS9XbavPiyfV20qy7Isv/OM/Cn7p+Rn3hFX5PQ32WV7U7+4OOuDFz4j2+1b5dO/Si9I5+Hh71/KbXTphPxwvjTe2CHb7Q/Ih+PiCiKihYPTAIiIALjcSg/qpasJRL7Vhk++2KqdS/rzKKIA1tesU72+agDRPsD24GqIbzOV3okilffhKTPiiPTGgZpHUJvpFR2KIgo31n0uN3EgNRCB86F12f/rueHO/sBBHteSSCQS0/hLKg+OERHdAAxWiYgA3H7HnQCA+IknEUAQL/x7cQuF6y7VDNP3B9EPoHbdKmC8Dd6DmUeZEhh8WwLWrcdq3cNTOWV3lQGS/g0ECgkj44B7VWU2OI5PxABUYsW9mW1S6P1r4JEvGr/9NfWrJIBPiIt1kvFBDL4znb/zMHr/ABHRXGCwSkQEoKxsKQBAGlqK1j/z6F/+X7kaqwB8nHkAa0pC6BsHkIAbVfcB0pl+/O7n0vNbpwYRHQDKqlfoH55SudP1b4H/IbyBIOt2fNIGXP1Vpg8ziaF3JABJJDNP/r9/AJ3L9uSdjxq/EAfWrcIKcYXAWe5B3ea6afwVPi8iotnEB6yIiABgIoTqqgBcHb9At/pVVSrS8Vo8/Mpt2LTOifjodfherEe0/klIn18H25Jm/OX3PHACwHgbKta0o+7Hv0Cw0DSAiRCqqyJoyTycJUj27cTKrf1w/dEmuC6O4BNPfhPO72xDp9OHR+5LIn6lDi+faITbsPc2hXDDUrz0uXw/IUtEtDAwWCUiAoCpBIbevgr3Q24l4MznWhKJXwHOMqfS+zqVQvLKddyW+T+g/GjAFcB5h65/ViCh7TPVGN13Gd2+PNtOpZC8kgR+swzOdA9q6koCSThRVij9a2Fs++39cL83htZycSUR0cLBYJWIaB4lw17c/ecefPjj5lkdWk+Gvbi7uwG/6PIVDr6JiCyOc1aJiOaR03cQwakDOPCuuGYGpkZx+DuXEGxjoKpxIYSNX+fPzxItNAxWiYjmlQstPUfxzy1+RGbpEfvR4JOIfqUDLdm3BhAAYApIZn+hjIgWCgarRETzzelBx+tfQM/zPTN+JVTqrTYcdp3Ej57K/PLWPEvFET0TwWgi/VaDqRQS70fQcyYKKblQ3taaQnygB5H3E9n3y6YujiJypgfRcb5zlmiuMVglIrKCu3w48t26GQ/b2z7fio4vWyRQRRyhrQHELnfBu3wjQm+F4d/wJNqlFHDhGDbeXYHAbE5/mCPxo9sQeP8SuuqXY+OhKMLNa/Hk8XNIIY5jm+5GxbeGxF2IaBYxWCUiorkxEELUcxAtD1XhdsQQeP4Svvnjk2jdUoe6XY3wIIHIkMFPIkylkLRKr+tUFKG3PDi4axOq7gBirQFc2v1TnAz4ULe5BY01QOLtwTw/7EBEs4HBKhERzQnpfaDu4TLg7AgklKHl5ZbcO2ETl3AZwG0G74iVDj2Iu+uOzSgATF0RfyI2gcSVq8B145+WzRsbn4sBmx9B2dQoRsaBsl3fVc0FTuDSPwG4TbvLTKTe3I21QfbUEqnx1VVERDSnhvYsxcYTjfjR5ZexKr0sFd6Gpf5B7Il+iNb7hR1SSSThhLPAa2QLSg2h/ZudiInLkyM4/fefRP3n9T+S4KptxZ6HCrw87N3dWLqpE439l/Fy5qd4r4Wx7bf9GHzmJ/hwX7HfCTOnZ4cDL312DD99Sp9HopsVg1UiIppDEg6srEbb73Xgcpcv/cMJKYS3LoV/dA+G/6EVmhm2qSSS/8+Z/QEEIIVkIvejC6krCVy35X4gYVomQqh+0Y3hdo+4pijpYDWqn3eh45fd2Z+3VQLuUezJ98ML15JIpG7T/HhD6koScDphM+hRBoawe+lWOPsNAniimxinARAR0dxJDGLgHFC5blXuF76u9eL1XsD1WD3cSKBzewBDAHCuHbu/04XA+gq0jQNACpHW3Tj8nY1Y3hzCgacD6BwYQNuGu7HzzXzj9nMhgcG3JeD+9VilCqJ7/6oHuKce9eVA4vg2BIbTq5JDCG2txc7Xohh88zls3BSCNJVE5Fs7cexVP+7ZHk6/QUBC22eUcx09vhM7tz6FdpRB+v5OBMIzmQRBtLgwWCUiorkzFEUUZdj0OdWw9sDfogduNPrcwLsH0HlvI1YhhfD3k2jaVwXbhRRwHUDiNCK/tQfNlXciGUuh7rtBNG/2of7B6xgcT6iPMremBhEdAMo+vw65sxjA3/YC7q/Uwz01hAM/cKOxGsCUhFDdoxjZchJHnqxD3ZYX8NXbAwgc+R6i/+5l1JddRyrzrtfEIIYurMLqe4EVXz6C1ofuBP5gF149fARBH6cBEGUwWCUiojkjxYYAZz02qYe1P/cF1N0ax+tBL9a+6MLRP3EDsKH2uRa4B06jc2kj6u8HcEc9XnjKhcGBKNZ/rTH9cFYcgz9JofLeGxjMnRvFEJyo/6J6XupqfKEWiJ9pg3dDG1z/WZnOkDi5G4F/asbTtZmXkCVw6RIQn3oUrZuTeP0HUdR5a5UpDQM9iK5bjxXpKQGDA1FtDzQRAZyzSkREc0o3BzVtKoXkldxc1IxIswMvffZD/GibE6klNtjEeZwX27Hx9wfw1V+eRC1SsC2ZRmhX8pzVFJJXAKdq7ml2zZUEri8pyz4MFml2wHtrNya/lz7GtTC8v70fK94bQ+sdnahd3oPG9LzXoT1L8dSdP8Hw11xILYnhuex5ppC6ZoNNLDOimxR7VomIaO7YDAJVALjVBqcQqOJaGJ2nVqH+YSfCL4SUV1ediyKKR7I9s/HuYxja0gjf1Xa0nZzm732VrUbLQ6X8YILNMFAFANsduUAVAG532uD+vVyvr/TnL2Fwy0E8Xa48cHWpPDPvdRT9fwWs/5wbkW+3IXZhEO+lNmH1/QDeDSGke5UB0c3rN7797W9/W1xIRER0w/36It569b8j9VvjuFT1x6hfbkOi90Xswx/i5Ufvw60Arn/Qi55/tOHi8C1o/Nbn8VvT6XL5l3dhxX23i0tn1V2rVyDWegB//69skLoCOParb6L/oAfOfwHAfgviB76Ld5baMfq9N/A/beeR+L8JXLynGU1rgJ+fiSJ1exxvXViPr//RXZwOQJTGaQBERGQd4rQBg3euppLp1z/lFllM+nVbdxi9okr9Ki5hekEqicT121BW8gtmiRYnBqtEREREZFnTGUAhIiIiIrqhGKwSERERkWUxWCUiIiIiy2KwSkRERESWxWCViIiIiCyLwSoRERERWRaDVSIiIiKyLAarRERERGRZDFaJiIiIyLIYrBIRERGRZTFYJSIiIiLLYrBKRERERJbFYJWIiIiILIvBKhERERFZFoNVIiIiIrIsBqtEREREZFkMVomIiIjIshisEhEREZFlMVglIiIiIstisEpERERElsVglYiIiIgsi8EqEREREVnWoghWI80OOJoj4mIiIiIiWuBuQLAaR2iNAw6H+s8PhpZEREREVMzcBqt9fjgcFeh8bAyTk5PZv7G2GLwOB/x94g5ERERERDlzGKxG4G8Iw902huFdLs0a165hTHb5EG5gDysRERER5TdnwWr8UADh8iC6hUA1q2YvguVhBA7Flf9PhFDtqEZoIgK/aspAdWZ9lnZ9vrmq8UPVmqkH6l5cZR0DZSIiIiKrm7NgVTorAZVu5AlVAbjgrkxvlyUhUNWFhsyUgS4fpFYvQhOZ9RH4HV7E2nLTCsbuC8B7SpVEOhiteK0RY5l0RoKINXDaAREREdFCM2fBKgC473OLizSM1vu6OuDJ/KdmL4LlEjp7ld5Vo95a165hdG/J/hdABPtbgeAPW3KB8rIWBLcA4TNKX6pr1zAmJ1XHISIiIiJLmtNgVdtrqqdf70aVPn7Nks5KcD9WV6C3FsCEhBgkBKq0byAQe1+JiIiIyPrmLFh13+cGYhLEGac5cUgx497VmfOhW/X2gexfO/tSiYiIiBaSOQtWXbuC8I0HsD/fPNG+/QiM+xDM9wBWHtJrPUIArAS9WcvcqEQYXfmOS0REREQLxpwFq4AHHV0+hBv0T/THD1XD0RDWzk81wfNsEO7xALyq9OKHvAiMa7ZCwxboXosVP+TPPqjFtwEQERERLQxzGKwCqOnA5GQ3KlsrNPNHM0/qd9SIOxSxrAXDI0FAlZ4X3cIDVoCnfRLdW8Lwqo95tgEty7TbEREREZG13SLLsiwuJCIiIiKygrntWSUiIiIimgEGq0RERERkWQxWiYiIiMiyGKwSERERkWUxWCUiIiIiy2KwSkRERESWxWCViIiIiCyLwSoRERERWRaDVSIiIiKyLAarRERERGRZDFaJiIiIyLIYrBIRERGRZTFYJSIiIiLLYrBKRERERJbFYJWIiIiILIvBKhERERFZFoNVIiIiIrIsBqtEREREZFkMVomIiIjIshisEhEREZFlMVglIiIiIstisEpERERElsVglYiIiIgsi8HqgheB3+FA9aG4uGKBiiO0ZjGdTwETIVQ7qhGaEFfMoT4/HA4/IuLyxWg+ytfQYrtHgUizA47mRVKL+vxwzKCeRJodcKwJYfFcXetZVPUtjfVmehisLnDxQwGEt3RjeJdLXEVEs2JmwSbvUSuLI/Q80D3SiM7HGTjQQqN07iy2QN4Ig9WFbCIEb2sluts94hqaBfFD1fzmS4g0e0sPNnmPWlr8kBedj+2FZ1kLgpUB7O8TtyjO0z6JyfdaUELtoAVldkf9ZqfeuNDyXjd8p7zwl1B3F5JFHawmYxH0DEhITSn/TyUlRM/0IPJ+IrtsYatD92QH+DFINFci6DrlRvDZUu8y3qOWVpv7EuJpn8Re9+wEIkQ3jgd729wIP7+4O1bmNVg9ceIEmpub0dzcjBMnToirZyT15k74f5DEYLAay58OIxqsxcbWKJK4jsg3lmPpo51IiDvNt+wcO2XY0ZH+036Ti8CfmV+1zJX9Vqad05P5BhhRhgjS6SjfvLRp67+Naddrexb16eb2Tw9HFEzbwEQI1ZpzlcQtiuQpDyFd7TBJ7htypNkoXWV9RasEjAdQoV6XmfPZ59fuIx7PII/xQ9Wq8qlGyOBUNfkxKMNi60Xa7Y3mqgpla7iNqNC1Lpxepp6q86XUb22a+t6LwunCZPlqtzE6jqCvC+HyRtQtyywoVndg8h6dSVmI11U731JMV72/eP7mhg+Fss+zT6E86Zhq66A/tkF7JN4D4ohI/vKIo+fxCk157Je0/VyZfbXlZlynNTLtg2G+obvGBcvLZFmJ11Zcb6hAPguml8lTn7rdS5eLJk39fVq8DTNT34q3ByJT55OvjCdCqHZUIDAOSK1KnSl0X2nOSVdWSl6N6o2Yjrg+3+ePq7YR7vFO9OSrQ4uBPE+OHz8uP/HEE5q/48ePi5uV6JL8F196Rv67X8tyf5Ndttt/R97xxtXs2vOvrJTt9h1yv2YfxcdXr8of/1pcOh3n5cOr7bLdbvy38pXz4g45Hx2WV9rt2ry9sUO221fKhz/KLOiXd2j+n17aZJftTZm9MnnIbaecs9EydTn0yzvsdnnHG9kFSrqrD8tKrvXppreSd9jVx8+di5nz1R1Ps1+xPBn46LC8UpPHdL515aNOVzmOOr/nX1mpP84bO5RyVJ+rLMv9TepyFI+Xv6w1ZfnRYXmHuryEa6/Lzxs7NOUi0pWTQf06/8oOzbXU7aNjcK1V+SiWXub6ZvOdKU/dMm0dK5auqfKV++UdBnW0WBlq67CZumPmHi29LMR0xG106aYpyw3qqYnrrbsvhDpQLE86BnVRv0+xe1/Jv3ie4n1iXB4G9digzdK3R/p7Wzx3pXwMrpnqXPX7aOu3htmyMjgXsWzU9Pk8Lx9uypRbkfQyedJ9NhgsE9Ip1MaZrW9ieYntgZ7J8ylYxsr5iJ9pYn3Lm7aQP7EOiHXE8P7M8/ljdK8sNvMWrDY1NemC1aamJnGz0nzcL//pc4OyLH8gv/RZu2z/0gk5F6rK8uDuT8l2+1P6YPXj1+Wtdrv8VERccYOIlVyWDW4QMx+E4j6yYSMgVvDzr6zU3wSawM+g8TG6WYsszxBvVoU278XzpNevCzAyN3mmITB3HuL/ZdmoATOm3TdPQ1LkPMT9jMsrj3xpa8rBQJH1hmVSiJCe/hyMroVR/RVo0i21fIsdxygYMsqvWC5m7lH9/43TFvL40WF5pe76aLcx/ODOVxb5lqflu96avJvIk46Jtq74vW90ffR5NioPcZt8y432LVyn89RFzfIiZSMyUVZ6xdbny2c+RvVQqDdGbWOR9kTMh1j+2a1094qg6HFERucjlodYhuL/ZXP3umHaRtvo09EtNypjWc6Tt8VlXqcBzBmbB8G2VUBiEAPngPW1m+DMrpTQ35cCatZjnWYnALY6vPqLyzjykLjiRnKjyi0umz2V9+afzi2dlYBTXu0wRFUA4miq+z5tBqWzEtyP1ekmiitDEyO6/RVxSDHAt7nwbD6zecpR0s0M1WT/GsLihrrzMK8S7uywcI56eKuiVZXDCQkx+NBQo946D810Ai/UufY8G4T7lLfwkGGGNAJJM3ydn2boyaCc1PJda7XppJdh5lrkTXc65asZPlSG9UphJr+lKpi2NAIJYXjVddvoPCrd2muUrz4sq0NjuYSRPDeUmettOk86hdu66d/7BQjlke+8DNsssSwLyVsXPWjYAsTOxZWHYvb50m1U8eFrReGyUkyjbufNp9o00ssybhs1CrRx+a6LkbztQV7FzsdMGQsmJMQgIVClrvsOeE+JGxZJe1r3p1EZu+CuFJctLvMWrD7wwAPiIsNlMzIURRRlWLWiLLds/DROXwDqvLWwqbcFkLqSBJy5palkAslU+j/XkkhcyfynEHEukvbP1DyieeRuG8Pk5KTwN4wW3c1x45SSJ1+XuP0kJufqQZd04+tFd/ZYY22FWiZjkWYHHFWdaBzJ5LcbPvUGy1owPDmJya7KdONoImgtSGm8K15rxFimjLo0R5ym2U4vY3bSVT7cvEC2bowhWC5utQCUB3PloPor6W0Fs2WO8lTKvb8g1HRgcnISY22xdJBvNmg1Ntt1e7bTyyjaxpky/fZgrs5H4UO3ro5OYpJvAJlV8xasbt++HRs2bIDNZoPNZsOGDRuwfft2cbMZGRroBbAJq+/PLZPOnEYcdXjkizbg3d3wn0wCSCLSuhvHXvXjnq1hpACk3gxg95ED2Ljcj9DBnQi8FsVA8EHc/fUICoesLrS8Z1Bx038zbbwV4jctpTdxptz3uSG91qN7MKiYfPvFezshlVehUNgWPiM20RJGVN9486Wdn/INU5/u3FHOM4ixgo1TGF3igwSSugcngq5TgK/LxAdxTUe6sZXQ2VugZAwm3MfPqSpKXxfC8KF7Gq9PKXg9SkjPFFPpFivfOHpek+BuG0NHwd6k2TI39yjcVaU9SJFvv4kedI4X7vXRX2/hXPKlPUMF65qK0luZI50t3veaL20zbVZBy9yoNKqL6ftbHNly7RpOB2xG+5hVQt3Om0+Ulp4p5to4/XUR6pup9kBtrs6nWDlOQ757yMT9qZil9sXC5i1YRTpgbW9vR3t7+6wHqoCE6NvicH8ckd444K2Hb0kCnS9ewhc2O4HxdkQrX0B92fX0K60SOB25E3uerMSdyRhStUcQfLIOPt96XP+5NM9vEVCGk9SvqYgf8hoMaUyfMgQWgFd8+4D4RKLAtSsI33gAFertJkLwtkrw7cvXoChDYRDeDxdp1g4LlZInz2Z9upgIwT/NXm3XvZWAOCRoQLdd+tyzlrUguAUIN6h7TyLwa4au3Kgq137wimURaZ5G70vNXgTLJQTULzoX8+WughsxSNkGUsyTnuG17vMrZV1CeqYUS9dU+SpfYtSBTPH7RtlHDIaKm7t7NDMsqLmuZurGshYEt0gIVKm3iyP0eADSlmDe4MHzbFB3/+nOpdQ8FVH83neh7jE3pNb9ueP0+Q2GYPUM63HRNsuM9GuEGsQ3NHgRLg9ibw2Ucm8W3w4wE6XUbaN8ZvJVSnpmFG/jTNW3Yu2Bzmydjz6d7L2uaXuA+CH/9Ea9Srw/c5ROnuyXoT6/wRsoFrZ5DVbnVCqG0XPA+tp1quF+F9bVuoGhY/A//CiGvnYUviVKRWndnMTrP4impweUob6tBa6hKKLrWtB4r7J3fCiK1HTmL80RT3u30tBm5segG91bxK1KsKwFwyNBQDPfM4Cqou+Y9KBjUnkxcXa/9FBPwW+yNR2Y7PIh3JCbJtG1WRieKSVPBuk6qkbQMN1e7Zq9CJan5+IVuvFrOtC9RTVn73EgKEwD8LRPardxdKFhJKjqwXGh5Yfa8+zaLA6RqfevQOdjYwV66l1oeW8MQeTqieNxoFs9XLasBd1tUM236kJDkeE0w2vdAGXuW0npmWAi3eLlm75vVPn2Ilh0KDBfD1wxc3aPGl1XhwOB+/YWneKiLyOlDhUcrjS4//TnUnqeCjI4tnjvu3Z15+5RhwOOMw0mp+AY1GMzbZYJrl3DGNPU1/QUIXVPYExdVsrw9EyOW0rd1uezAp33KfNFS0mvOBNtnME119U3E+2BaLbOJ/fcQG5Kn/6+cqDibIOJAFNLn46J+zNjQkJMNS82ciZseu7vQnGLLMuyuHCxSF1JAnc49XNTkwlcv7UMziWqhRMhVK+J4+V/egGrr9tgWwIM7VmKrb/5Jj7ctwJAAu0blmPga5dx8otAaolNly4RLTITIVRXdaJxpPDQJRHRfIkfqlbm8L7XAhfiCK3xAj9cXG3W4u1ZBWAzCFQBwOYUAlUA0mvtiG+rxfpzIbS9kUpPIwAe8axQNpg4jWPDPjRuTqL9hU4ktbsT0WKUGZ57cSYD2kREcyWC/ZrpKxJGYPBmgQVuUQer02G73Ymyq0No+74TjZtt6ddeebA68zqI25y48x5gtLUN17/SDNX7BYhoEcsMIVr9TR5EdLNRelHDW7pV00g86DD98NnCsainAUyXdtpACskk4FS9ygqpJJJwqt9uRURERERziMEqEREREVkWpwEQERERkWUxWCUiIiIiy2KwSkRERESWxWCViIiIiCyLwSoRERERWRaDVSIiIiKyLAarRERERGRZDFaJiIiIyLIYrBIRERGRZTFYJSIiIiLLYrBKRERERJbFYJWIiIiILIvBKhERERFZFoNVIiIiIrIsBqtEREREZFkMVomIiIjIshisEhEREZFlMVglIiIiIstisEpERERElsVgddGKI7TGAUdzRFyxYEWarXA+EfgdDlQfiosrgHnJo3Kd8+Vn0enzw+Hw40aWsKE+PxyOaoQmxBUL1EQI1YvofOKHqmdQTxZf22k9C7vduvHtPDFYXaz69iOAIMbaPeIamoFIsxextjEM73KJq2ghmQih2uGAv09cYUYcoefD8HUNo2WZuI7mXwT7X2vEWBfgZUBBc4pfbG4UBquLUgT+hhiCP2wBQ6pZNBFCIBZENwPVBS6O0OMBoG0MHTXiuuLih7wIVHaXtC/NvUizF9jXAlfNXgRjgRJ6i11oeW8Sk/yiv8Apo2ClfSE1y4WW97rhO+Wd4+PQog5Wk7EIegYkpKaU/6eSEqJnehB5P5FdtihNuLF3kr0+s68O3e/xC8CCN9GDznEfgqV+6ajtZiBjWXG4n51Mf5FQAok6cROiWeXB3jY3ws+HsDAnNSwM8xqsnjhxAs3NzWhubsaJEyfE1TOSenMn/D9IYjBYjeVPhxEN1mJjaxRJXEfkG8ux9NFOJMSdLCwzR0aZi+VI/6nnZCnDEf4+AMtc2YAqfqgajjWqmygz56/Pn0snvV6TtnqfNO2xhW+sYrqq/cX9zA2ZKN+Ki+0jpl302636vIV0xbS086niCD1eAa9mmZk8Ctvo5tGlh5Gy64vMG0wPX+fyKIlb6LYxupZ6Yj5V+SiYXmbuWURzHsp10KYpXpvC5a2INKvzJJad0Tb644jivZ2QtjQgG25m52tq86vJT58/e96uZZm7S+i5yaTTpy6vdJ419c7oPITyL1LGuXMscN0K0JZ9NUIG1UiXdrF6lGkDhPqiux5ifdLcNxH4Dc5BO0ewUHlI2F+lLg8venLJqPaNa+uNYXkL97/mPjVxXoXKy2RZTbduQ3dt1eUm1hWhHoptuOocCn82FEkX+rIxarfEfBu1B1oFjtvnh8PhRRhAuEFZn2uTitUvfX7156zlqm2Ee7wTPSbuPSqRPE+OHz8uP/HEE5q/48ePi5uV6JL8F196Rv67X8tyf5Ndttt/R97xxtXs2vOvrJTt9h1yv2afG+CNHbLdbs/zVzg/ynnY5ZWvnE8vOS8fXm2X7U2ZvZT/73hDtVPmXFcfljN7ZfOQ3a9f3pHJg7AsdyyDdD46LK+0q46nS1eh5Ft9bul8q9PSyXN8If2ieRJ9dFheaV8pH/4os6Bf3qE+Z3XedWkp+c7lyWwed6iOly4PVZ77mwpvr6HLk1G9kOX+JoPyFq6LRjpd7bnk8lE4vfT/VeWaLQfdMnU6xcpbX1aZbTTpfHRY3qHKt1IP1ddYZHCfGKUrpvPGDoM6q9SBbFqZdLLbZcrGYJnmegjp6M5dX8aybHzdMvdh3nsg37UQrlfxPBnItAGqbXTH0t2DYnn0yzvE89TdJzMpj9w1yS0T72Xje11zzXTHEsvrvHy4aeZlNb26bdTe5u4zsW3RXU9dG272s6FIuvnubbH8DMq3cD0uclzdNcks05eh2A4XbvP02xsfi2bTvAWrTU1NumC1qalJ3Kw0H/fLf/rcoCzLH8gvfdYu2790Qs6FqrI8uPtTst3+VMHgcHrOy39Rt1U+fUlcPnv0N2KmccncVAYfwpkGUNcgaQNjXSOp28/EDW7UkOo+mIosT9PlOU3bQJjIk8jg3PMTP7C0/zeXRwMG10zz4VqAcdrF08iX1wzjdPPTpmd0fP0HWvHGXEgnXx0peg2LHceg3hh+MAr5MR2sCmkb3RfCOZx/ZaW+/DVp6T8s5QLXLd9yRZ7yEfJePE8GjM5VOF5/k1gvxPIwuD66c5pJeRjvW6hO57t/NMuLlY3IRFnpFVlfUh7EL2gGnwNGywzKI0tIx/i6GLUbasXWG9Dl36i8zNQvPfGc9duXkF+alnmdBjBnbB4E21YBiUEMnAPW126CM7tSQn9fCqhZj3WanWbg3Gkce/t2OMvEFbOs0j138yXLq+AWl2VMSIhBQkAzvOaA95S4YSXc6nmy0gik8kbUiXNnl9WhsVzCiH4kCAAgnZXgfqyu8LmazpNKzV4Ey8Pw5h1iUg8rVSAwLq7PMZXHNM3wVkNYtcaFln0+SK0VxkNnGnFIMcC32dxcSfXwYUVrnoIGTKdrPr2cynuLlU6B8s5Xd4xohuyUob/pc6Mq7w0wU8J9IZDOSsApr6YeO6oCEEvZfZ86g/mvm2ezD4hJxsOWExJi8KGhyMNhZvOkV+hclTwr9V2VruaeMG8m5aHdt7B897oy9DuilMmyFgS3pNsjw+lARgqVVdp06raJeyZ/W1RAoc+GtPzp5r8uegXagzzyH3fmptfmueCuFJfRbJq3YPWBBx4QFxkum5GhKKIow6oVqihy/DROXwDqvLWwqbfNSiGZSCCZUi9KIqFZoJV4ZwCSmeBXnC+p+SsWrMw3H7onJzEp/s3rgybTzVP6Kd/JblSmPzAzQavS6HmBrkw6YwiWi/tPl9L4VrzWiLFM3rp82k1qOjA5OYmxthi8s1EP0h9uXnRny2OsrdhHTQGznV7abJV3pNkBR1UnGkcy6XRDKOEFwd02pq/H8/yQ5FzlyZe95uq/jtwc4gXK056ux7F0kG86aDU2u3XbRFtUktlJd/rtwewc19ActXk0M/MWrG7fvh0bNmyAzWaDzWbDhg0bsH37dnGzGRka6AWwCavvzy2TzpxGHHV45Is24N3d8J9MZtcleneidusBRN4ZROdX12JnXxI4F8LO57vQtqEabelveqnwNji2hpFKRHDg6Z3YGozClYpi99MhDF3LHUsnHZgY/81OYx07p+1Pkc4W+0ZowjI3KhFGl4nJ/RruKuNJ5xM96Bwv3JMlvdYj9Awp39CzSs0TAMCDjnQDpBwnjp7XJLin+Sqjonns60IYPlNvEHDtGk5/GBU+p/AZ8QNQwoiqByLe2wmpfDrv11V6BPTpKqafnhkmy9ug7sTPqQs4gq5TuHHvO830omVMSFDnplTu+zL1cDryX7fImXCRURiDOiZpz620PBWTP89a4qiLcF8Zyp928fIoLF9ZKPeG2OuY/kLc5QNOdc3gi2cJdTtfe4vptUXTYjJd/XVRt1sm2wM1k8c1Vrh+ldbmmamjNBPzFqwiHbC2t7ejvb191gNVQEL0bXG4P45Ibxzw1sO3JIHOFy/hC5uVCQLJPj9WBl14+UQrfJvr0PKNTeh/+rs4+P0UWv6sCrYLKeC6kkpsKAL3qlWwlXmw53AjqpJlqP/2ERw53IJVS7IHu8FcqHvMDal1v+aJyILD4qZ50LAFCDdoe/3ih/y6pyo1skNjwlsLHg9A2hLM2wh7ng3CPR7QPHkfP+QVhoVKyFOfP8/TtMoHnTqw1x9Py1Qe3VVwIwYpm58I/MIQWai58FOmOcqUAQjv84s0a4cGXfdWaoOqiRC8RYawPM8G4T7l1UyNyJRjKekVZ6K8a/YiWC4h8LiqfHTHdqOqXPsFTSwPPTeqCkxByaumAT6EEciWUboeC5uVQhlO1tYlIAJ/kZ45o+um3PNuBJ/N80G7rAVB3X0j1svS81SMZ7O+DmMiBH/2OOn7WvUaIF3dyKOk8jDBtSsI33gAFcLT4t5WCb596WBJcw6zoYS6bdjepq9Z0baoREXTNdNumWgPREWPC8MyNFO/SmvzlOA7M/VJ6Sme4SgZacxrsDqnUjGMngPW165TDfe7sK7WDQwdg//hRzH0taPwLQGAIbRtD2P1My1w35re9H8nkEj8L7ifa4F74DQ6lzai/n4AGEX/XwHrV6W/z52LIprS9t7OF9eu7uycTIfDAceZhlkbvvC0T6J7iypthwMVZxvyBpwZ+v0q0PnYWIGheqXRHR4JAqq5bV50o3uLdjN92sXzlHmNiSMzhJT+Zu5pV17snDtesPAwlJk8LmtBdxtU82q70CAOVcUCqMjmXxkGy9u7UNOByS6f5hy6NgvDZTUd2jJ5HAgWqwMG51LxWpUy962U9EwoXt4utLw3hiBU5fM40K0pPxdafqjNd9fmYkOl+XvgCvOgoyszvzh9rX5Y7FgmGZS/wxFAVbEAy2i/BqC7yFC9/r7pQsNIUNtDaJS2mTwVY1CHHVUjaFC989bT3q0Eh9m6ob/3DRnl2UR5FOdBx6S2vmaG59X3akx33JmMlpVSt42urRfY7DHXFpXCTLoG11xst4q3BwIzx9U8E5B77VfR+lVKmzchITan897pFlmWZXHhYpG6kgTucOrmpqaSCVy/tQzOTC/oRAjVVV1o/IefouUeZdHo88vx4Ln/hMv/1YfYnqXY+ptv4sN9K4BEJ2qXR9H0f47Cc92GZLgWy/sacbnLB9u1FFJLbLrjEZGF9PlnIZggIlLED1VrOj9o9i3enlUANoNAFQBsTlWgCgBLnLgTbrg+nf5/sgeHXy1DcJ8PNgBXkymsW7UCAJAa6EF03XqsvnAMz4UTGB2KonLdKtiQQOcrp5GbAUtElpR+K0RuSJ+IqFQR7FdPCaE5sah7VqdDOroR/rP1+Oa664iEB1HZ9iqay5VQN3GyFmt71+HgQwlEfn4Jg2fvRN2K27D6z4JwH1+LrR/tQvMdH+ATX26F7y4xZSKynIkQqqsCqCw07YKIqKA4QmsqEKjkTzDPNQarateSSEzdhjKnQX/stSSScCo9steSSP6GE5nNUleSgNMJW2a+KxERERHNCgarRERERGRZi3rOKhEREREtbAxWiYiIiMiyGKwSERERkWUxWCUiIiIiy2KwSkRERESWxWCViIiIiCyLwSoRERERWRaDVSIiIiKyLAarRERERGRZDFaJiIiIyLIYrBIRERGRZf1/wUz1tD3tJDQAAAAASUVORK5CYII=)\n",
        "\n",
        "Essa abordagem de inicialização de pesos é eficaz para redes que utilizam funções de ativação simétricas como Sigmoid e Tanh."
      ],
      "metadata": {
        "id": "u-FKl7B6WGQH"
      }
    }
  ]
}
